# Self-Healing
- 컨테이너가 제대로 동작하지 않을 때, 컨테이너 재시작.
- 노드가 죽었을 때, 재스케줄링 및 재배치
- 건강한 컨테이너로 어플리케이션을 서비스하는 걸 보장.

# livenessProbe
- Pod 가 계속 실행할 수 있음을 보장.
- Pod spec에 정의
 
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx-pod
spec:
  containers:
  - image: nginx:1.14
    name: test-server
    livenessProbe: # livenessProbe 설정
     httpGet: # 아래 정보로 주기적으로 요청. 응답 실패 시 건강하지 않은 컨테이너.
      path: /
      port: 80
```

- 어떤 어플리케이션이냐에 따라 확인 방법이 다를 수 있음.

```
✔ lineness probe
liveness : 살아있음
probe : 검진
```

## livenessProbe 매커니즘
- pod 가 아니라 container 를 재시작 한다. ip 는 바뀌지 않음.

### httpGet
- 지정한 ip, port, path 에 GET 요청을 보내 컨테이너 상태 확인
- 응답 코드가 200이 아닌 경우, 컨테이너를 재시작.
    - 도커 허브에서 건강한 컨테이너를 다시 받아서 실행
    - ❓ yaml 에서 따로 설정한 것들이 있다면?

### tcpSocket
- 지정된 포트에 TCP 연결 시도.
- 연결되지 않는 경우 컨테이너 재시작.

```yaml
livenessProbe:
 tcpSocket: 
  port: 22
```

### exec
- 명령을 전달하고 명령의 종료코드가 0이 아니면 컨테이너 재시작.

```yaml
livenessProbe:
 exec: 
  command:
   - ls
   - /data/file
```

## livenessProbe 매개변수
| 매개변수 | 설명 | Default | Min |
|--|--| --|--|
| initialDelaySeconds| Pod 실행 후 delay 할 시간 (초)| 0 | 0 |
| periodSeconds | health check 반복 실행 시간 (초) | 10 | 1|
| timeOutSeconds | health check 후 응답을 기다리는 시간 (초) | 1 | 1|
| successThreshold | Probe 실패 후 성공한 것으로 간주되기 위한 최소 연속 성공 횟수| 1 | 1|
| failureThreshold | 연속으로 해당 횟수 만큼 실패한 경우 실패라 간주. </br> 실패라 간주하면 해당 컨테이너를 재시작| 3 | 1 |
| terminationGracePeriodSeconds | kubelet 이 실패한 컨테이너에 대해 종료를 트리거한 것과 컨테이너 런타임을 강제로 중지하기 까지의 유예 시간 | 30 | 1 |



### See also
- https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes


### 실습
```yaml
# liveness-pod yaml 예시 파일
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: liveness-pod
spec:
  containers:
  - image: nginx:1.14
    name: nginx-c
    livenessProbe:
     httpGet:
      path: /
      port: 80
```

```bash
# -f 인자로 생성하는 커맨드
kubectl create -f fileName.yaml
```

```bash
>>> kubectl describe pod liveness-pod


Name:             test-pod
Namespace:        default
...
# describe 했을 때, `Liveness` 를 확인할 수 있음. 설정하지 않은 값들에 대해서는 기본 값으로 설정됨.
Liveness:       http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
...
```

```bash
# 실행 중인 pod 의 yaml
kubectl get pod liveness-pod -o yaml
```

## Example
- `smlinux/unhealty` 이미지 사용하여 테스트
  - 5번 째까지는 healty, 이후부터 unhealty 응답하는 이미지.

```yaml
# livenessProbe 테스트 yaml
apiVersion: v1
kind: Pod
metadata:
  name: unhealthy-test
spec:
  containers:
  - image: smlinux/unhealthy
    name: unhealthy-c
    livenessProbe:
     httpGet:
      path: /
      port: 80
     initialDelaySeconds: 3
```

```bash
>>> kubectl describe pod unhealthy-test

Type      Reason    Age               From
---       ---       ---               ---
Normal   Scheduled  29s               default-scheduler  Successfully assigned default/unhealthy-test to gumo-pg-2
Normal   Started    41s                kubelet            Started container unhealthy-c
Warning  Unhealthy  15s (x3 over 35s)  kubelet            Liveness probe failed: Get "http://10.44.0.1:80/": dial tcp 10.44.0.1:80: connect: connection refused
Normal   Killing    15s                kubelet            Container unhealthy-c failed liveness probe, will be restarted
Normal   Pulling    22s (x2 over 92s)  kubelet            Pulling image "smlinux/unhealthy"
Normal   Created    21s (x2 over 79s)  kubelet            Created container: unhealthy-c
Normal   Started    21s (x2 over 78s)  kubelet            Started container unhealthy-c
Normal   Pulled     21s                kubelet            Successfully pulled image "smlinux/unhealthy" in 1.372s.
Normal   Killing    8m6s (x5 over 12m)      kubelet            Container unhealthy-c failed liveness probe, will be restarted
Normal   Pulled     7m35s                   kubelet            Successfully pulled image "smlinux/unhealthy" in 1.379s.
Warning  Unhealthy  5m6s (x19 over 12m)     kubelet            Liveness probe failed: Get "http://10.44.0.1:80/": dial tcp 10.44.0.1:80: connect: connection refused
Warning  BackOff    2m26s (x18 over 6m36s)  kubelet            Back-off restarting failed container unhealthy-c in pod unhealthy-test_default
Normal   Pulling    92s (x8 over 12m)       kubelet            Pulling image "smlinux/unhealthy"

```
- 제대로 동작하지 않는다면 Restart 하여 Pod 관리.

## 문제
- 컨테이너에 /tmp/healthy 파일이 있는지 5초마다 확인
- Pod 실행 후 10초 후부터 검사
- 성공 횟수는 1번, 실패 횟수는 연속 2회로 구성
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-exam
spec:
  containers:
  - name: busybox-container
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - ls
        - /tmp/healthy
      initialDelaySeconds: 10
      failureThreshold: 2
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
```